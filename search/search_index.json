{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Your AI Coding Partner, On Your Terms.","text":"<p> Local-First \u2022 Sandboxed \u2022 Model-Agnostic </p> <p>     Dodo is an open-source AI agent that lives in your terminal. It helps you build, refactor, and understand code without sending your IP to the cloud (unless you want to).   </p> <p> Get Started View on GitHub </p> <p></p>"},{"location":"#why-dodo","title":"Why Dodo?","text":"<p>Most AI coding tools are either: 1.  Closed Source SaaS: You have no control over data privacy or internal logic. 2.  IDE Plugins: Great for autocomplete, but bad at \"Big Picture\" agentic tasks.</p> <p>Dodo is different. It's a standalone agent engine that runs locally on your machine.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p> Sandboxed Execution     ---     Every command runs in an isolated Docker container. Let the agent run tests or builds without fear of <code>rm -rf /</code>.</p> </li> <li> <p> Model Agnostic     ---     Bring your own keys. Works with GPT-4o, Claude 3.5 Sonnet, Gemini Pro, or Local Models (Ollama/LM Studio).</p> </li> <li> <p> Semantic Indexing     ---     Dodo indexes your codebase into a local vector database, allowing it to \"understand\" project structure and find relevant files instantly.</p> </li> <li> <p> Terminal Native     ---     Built for developers who live in the CLI. Full keyboard control, beautiful TUI (Ink), and integrates with your existing workflow.</p> </li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>graph LR\n    subgraph Your Machine\n        UI[Terminal UI] &lt;--&gt; Engine[\"Dodo Engine (Go)\"]\n        Engine &lt;--&gt; Index[SQLite Vector DB]\n        Engine &lt;--&gt; Docker[Docker Sandbox]\n    end\n\n    Engine &lt;--&gt; API[LLM Provider API]</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install via npm:</p> <pre><code>npm install -g dodo-ai\n</code></pre> <p>Start Coding </p>"},{"location":"ENGINE_ARCHITECTURE/","title":"Engine Architecture - Complete Guide","text":""},{"location":"ENGINE_ARCHITECTURE/#overview","title":"Overview","text":"<p>The engine (<code>internal/engine/</code>) is a ReAct (Reasoning + Acting) loop implementation that orchestrates LLM calls and tool execution. It's designed to be provider-agnostic (works with OpenAI, Anthropic, etc.) and framework-independent (no Eino dependency).</p>"},{"location":"ENGINE_ARCHITECTURE/#core-architecture","title":"Core Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    engine.Run()                         \u2502\n\u2502  Main loop: steps until Done or MaxSteps reached      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  stepOnce()                             \u2502\n\u2502  1. Detect phase                                        \u2502\n\u2502  2. Build messages (with processors)                   \u2502\n\u2502  3. Call LLM                                            \u2502\n\u2502  4. Execute tools (parallel)                           \u2502\n\u2502  5. Append results to history                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ENGINE_ARCHITECTURE/#key-components","title":"Key Components","text":""},{"location":"ENGINE_ARCHITECTURE/#1-state-statego","title":"1. State (<code>state.go</code>)","text":"<p>The <code>State</code> struct holds all execution context:</p> <pre><code>type State struct {\n    History   []ChatMessage  // Full conversation history\n    Step      int            // Current step number\n    Done      bool           // True when LLM returns no tool calls\n    Phase     Phase          // Current phase (explore/discover/edit/validate)\n    Model     string         // Model name (for provider)\n    MaxSteps  int           // Maximum steps before stopping\n    BudgetTok int           // Token budget (soft limit)\n    Totals    Usage         // Accumulated token usage\n}\n</code></pre> <p>Key Methods: - <code>Append(msg ChatMessage)</code> - Adds message to history</p> <p>Design Notes: - State is mutable - modified in-place during execution - History grows indefinitely (processors handle truncation) - Phase is auto-detected from tool usage patterns</p>"},{"location":"ENGINE_ARCHITECTURE/#2-llmclient-interface-typesgo","title":"2. LLMClient Interface (<code>types.go</code>)","text":"<p>Abstracts provider-specific LLM calls:</p> <pre><code>type LLMClient interface {\n    Chat(ctx, model, messages, toolSchemas, opts) (LLMResponse, error)\n    Stream(ctx, model, messages, toolSchemas, opts) (&lt;-chan StreamEvent, &lt;-chan error)\n}\n</code></pre> <p>Why This Design: - Provider-agnostic: Engine doesn't care if it's OpenAI or Anthropic - Simple contract: Just messages in, response out - Tool support: Schemas passed separately (provider converts to their format)</p> <p>Current Implementations: - <code>providers.OpenAIClient</code> - Direct OpenAI SDK calls - <code>providers.AnthropicClient</code> - Direct Anthropic SDK calls</p>"},{"location":"ENGINE_ARCHITECTURE/#3-tool-system-toolsgo","title":"3. Tool System (<code>tools.go</code>)","text":"<p>Simple, direct tool execution:</p> <pre><code>type ToolFunc func(ctx context.Context, args map[string]any) (string, error)\n\ntype Tool struct {\n    Name        string\n    Description string\n    SchemaJSON  string  // JSON Schema for parameters\n    Fn          ToolFunc\n}\n\ntype ToolRegistry map[string]Tool\n</code></pre> <p>Design Philosophy: - Simple: Just a function that takes args and returns a string - No abstraction: Direct execution, no middleware - JSON Schema: Standard format for all providers</p> <p>Tool Execution Flow: 1. LLM returns <code>ToolCall</code> with name and args 2. Engine looks up tool in registry 3. Calls <code>Tool.Fn(ctx, args)</code> 4. Result appended to history as <code>tool</code> message</p>"},{"location":"ENGINE_ARCHITECTURE/#4-react-loop-rungo-stepgo","title":"4. ReAct Loop (<code>run.go</code> + <code>step.go</code>)","text":"<p>The core execution loop:</p> <pre><code>func Run(ctx, llm, reg, st, hooks, opts) error {\n    for st.Step = 0; st.Step &lt; st.MaxSteps &amp;&amp; !st.Done; st.Step++ {\n        if err := stepOnce(...); err != nil {\n            return err\n        }\n    }\n    if st.Done {\n        hooks.OnDone(ctx, st)\n    }\n    return nil\n}\n</code></pre> <p>Step Execution (<code>stepOnce</code>):</p> <pre><code>1. Detect Phase\n   \u2514\u2500&gt; Analyze history to determine current phase\n\n2. Build Messages\n   \u2514\u2500&gt; Apply processors (summarize, truncate, etc.)\n\n3. Call LLM\n   \u2514\u2500&gt; Convert engine.ChatMessage \u2192 provider format\n   \u2514\u2500&gt; Call provider SDK\n   \u2514\u2500&gt; Convert response \u2192 engine.LLMResponse\n\n4. Update State\n   \u2514\u2500&gt; Append assistant message\n   \u2514\u2500&gt; Update token totals\n\n5. Execute Tools (if any)\n   \u2514\u2500&gt; Run all tools in parallel (goroutines)\n   \u2514\u2500&gt; Append results in order\n\n6. Continue Loop\n   \u2514\u2500&gt; If no tools \u2192 Done = true\n   \u2514\u2500&gt; Otherwise \u2192 next iteration\n</code></pre> <p>Parallel Tool Execution: - All tools run concurrently using <code>sync.WaitGroup</code> - Results collected in order-preserving slice - Errors wrapped as <code>\"ERROR: ...\"</code> in content</p>"},{"location":"ENGINE_ARCHITECTURE/#5-message-processors-processorsgo","title":"5. Message Processors (<code>processors.go</code>)","text":"<p>Transform message history before LLM calls:</p> <pre><code>type Processor func(ctx, st *State, msgs []ChatMessage) ([]ChatMessage, error)\n</code></pre> <p>Built-in Processors:</p> <ol> <li><code>KeepLastN(n)</code> - Keep only last N messages</li> <li>Simple truncation</li> <li> <p>Used to limit context size</p> </li> <li> <p><code>TruncateLongTools(maxChars)</code> - Trim large tool outputs</p> </li> <li>Keeps head + tail, removes middle</li> <li> <p>Prevents huge tool outputs from bloating context</p> </li> <li> <p><code>SummarizeOlderThanN(llm, keepLastN)</code> - Compress old history</p> </li> <li>Uses LLM to summarize old messages</li> <li>Keeps recent messages intact</li> <li>Falls back to truncation if summarization fails</li> </ol> <p>Processor Pipeline: <pre><code>msgs = ApplyProcessors(ctx, st, msgs,\n    SummarizeOlderThanN(llm, 12),  // Summarize old\n    KeepLastN(8),                   // Keep last 8\n    TruncateLongTools(4000),        // Truncate large outputs\n)\n</code></pre></p> <p>Design Notes: - Processors are composable - chain multiple together - Each processor receives full message list - Processors can be stateful (access <code>st</code>)</p>"},{"location":"ENGINE_ARCHITECTURE/#6-hooks-system-hooksgo-multi_hookgo","title":"6. Hooks System (<code>hooks.go</code> + <code>multi_hook.go</code>)","text":"<p>Observability and side effects:</p> <pre><code>type Hook interface {\n    OnStepStart(ctx, st *State)\n    OnBeforeLLM(ctx, st, messages)\n    OnAfterLLM(ctx, st, resp)\n    OnToolCall(ctx, st, call)\n    OnToolResult(ctx, st, call, result, err)\n    OnHistoryChanged(ctx, st)\n    OnSummarize(ctx, st, before, after)\n    OnStreamDelta(ctx, st, delta)\n    OnDone(ctx, st)\n}\n</code></pre> <p>Hook Lifecycle:</p> <pre><code>Step Start\n  \u2514\u2500&gt; OnStepStart\n\nBefore LLM Call\n  \u2514\u2500&gt; OnBeforeLLM\n\nAfter LLM Call\n  \u2514\u2500&gt; OnAfterLLM\n  \u2514\u2500&gt; OnHistoryChanged (assistant message added)\n\nTool Execution (for each tool)\n  \u2514\u2500&gt; OnToolCall\n  \u2514\u2500&gt; OnToolResult\n  \u2514\u2500&gt; OnHistoryChanged (tool result added)\n\nStep Complete\n  \u2514\u2500&gt; (if Done) OnDone\n</code></pre> <p>Multiple Hooks: - <code>Hooks []Hook</code> - Slice of hooks, all called in order - Each hook method called for each registered hook - Use <code>NopHook</code> to implement only needed methods</p> <p>Example Uses: - Logging (<code>LoggerHook</code>) - Metrics collection - TUI updates (<code>TUIHook</code>) - Progress tracking</p>"},{"location":"ENGINE_ARCHITECTURE/#7-phase-detection-phasego","title":"7. Phase Detection (<code>phase.go</code>)","text":"<p>Infers current workflow phase from tool usage:</p> <pre><code>func DetectPhase(history []ChatMessage) Phase {\n    // Look backwards through history for tool names\n    // Return phase based on tool type\n}\n</code></pre> <p>Phases: - <code>PhaseExplore</code> - Initial exploration - <code>PhaseDiscoverAndPlan</code> - Finding code, reading files - <code>PhaseEdit</code> - Making changes (search_replace, write) - <code>PhaseValidate</code> - Testing (run_tests, run_build, run_lint)</p> <p>Detection Logic: - Scans history backwards for <code>tool</code> messages - Maps tool names to phases - Returns first match found</p> <p>Limitations: - Naive: Only looks at tool names - No context: Doesn't understand tool sequences - Last-wins: Returns phase of most recent matching tool</p>"},{"location":"ENGINE_ARCHITECTURE/#8-budget-management-policy_budgetgo","title":"8. Budget Management (<code>policy_budget.go</code>)","text":"<p>Token budget enforcement (soft limit):</p> <pre><code>func BuildMessagesWithBudget(ctx, st, llm, estimate, processors) ([]ChatMessage, error) {\n    // Estimate tokens\n    // If over budget, summarize old messages\n    // Return compressed messages\n}\n</code></pre> <p>Current State: - Placeholder: <code>estimate</code> function not implemented - Naive: One-pass summarization attempt - Soft limit: Doesn't hard-fail, just compresses</p> <p>Missing: - Real tokenizer integration - Multi-pass compression - Hard budget limits</p>"},{"location":"ENGINE_ARCHITECTURE/#data-flow","title":"Data Flow","text":""},{"location":"ENGINE_ARCHITECTURE/#message-flow","title":"Message Flow","text":"<pre><code>User Input\n  \u2514\u2500&gt; ChatMessage{Role: \"user\", Content: \"...\"}\n  \u2514\u2500&gt; st.Append() \u2192 added to History\n\nLLM Call\n  \u2514\u2500&gt; buildMessagesForCall()\n      \u2514\u2500&gt; Apply processors\n      \u2514\u2500&gt; Return processed messages\n  \u2514\u2500&gt; llm.Chat(messages, toolSchemas)\n      \u2514\u2500&gt; Provider converts to their format\n      \u2514\u2500&gt; SDK call\n      \u2514\u2500&gt; Provider converts response\n  \u2514\u2500&gt; LLMResponse{\n          Assistant: ChatMessage{...},\n          ToolCalls: [...],\n          Usage: {...},\n      }\n  \u2514\u2500&gt; st.Append(resp.Assistant)\n\nTool Execution\n  \u2514\u2500&gt; For each ToolCall:\n      \u2514\u2500&gt; reg[call.Name].Fn(ctx, call.Args)\n      \u2514\u2500&gt; Result string\n  \u2514\u2500&gt; ChatMessage{Role: \"tool\", Name: \"...\", Content: result}\n  \u2514\u2500&gt; st.Append(toolMessage)\n\nNext Iteration\n  \u2514\u2500&gt; History now includes: user \u2192 assistant \u2192 tool \u2192 ...\n  \u2514\u2500&gt; LLM sees full context\n</code></pre>"},{"location":"ENGINE_ARCHITECTURE/#state-mutation","title":"State Mutation","text":"<pre><code>Initial State:\n  History: [system, user]\n  Step: 0\n  Done: false\n\nAfter Step 1:\n  History: [system, user, assistant, tool]\n  Step: 1\n  Done: false (tools were called)\n\nAfter Step 2:\n  History: [system, user, assistant, tool, assistant]\n  Step: 2\n  Done: true (no tools called)\n</code></pre>"},{"location":"ENGINE_ARCHITECTURE/#features","title":"Features","text":""},{"location":"ENGINE_ARCHITECTURE/#what-works-well","title":"\u2705 What Works Well","text":"<ol> <li>Simple API</li> <li>Single <code>Run()</code> function to execute</li> <li>Clear state management</li> <li> <p>Easy to understand flow</p> </li> <li> <p>Provider Independence</p> </li> <li>Works with any LLM via <code>LLMClient</code> interface</li> <li>No framework dependencies</li> <li> <p>Easy to add new providers</p> </li> <li> <p>Parallel Tool Execution</p> </li> <li>All tools run concurrently</li> <li>Significant performance improvement</li> <li> <p>Order-preserving results</p> </li> <li> <p>Extensible Hooks</p> </li> <li>Multiple hooks supported</li> <li>Comprehensive lifecycle events</li> <li> <p>Easy to add logging/metrics</p> </li> <li> <p>Message Processing</p> </li> <li>Composable processors</li> <li>Built-in summarization</li> <li> <p>Tool output truncation</p> </li> <li> <p>Phase Detection</p> </li> <li>Automatic phase tracking</li> <li>Useful for UI/progress display</li> </ol>"},{"location":"ENGINE_ARCHITECTURE/#weaknesses-limitations","title":"\u26a0\ufe0f Weaknesses &amp; Limitations","text":"<ol> <li>No Error Recovery</li> <li>Tool errors wrapped as strings</li> <li>No retry logic</li> <li> <p>LLM errors stop execution immediately</p> </li> <li> <p>Naive Phase Detection</p> </li> <li>Only looks at tool names</li> <li>No understanding of sequences</li> <li> <p>Can misclassify phases</p> </li> <li> <p>Budget Management Incomplete</p> </li> <li>No real tokenizer</li> <li>Single-pass compression</li> <li> <p>No hard limits</p> </li> <li> <p>No Streaming Support</p> </li> <li><code>Stream()</code> method is placeholder</li> <li>No incremental output</li> <li> <p>Can't show progress during generation</p> </li> <li> <p>State Mutability</p> </li> <li>State modified in-place</li> <li>Hard to test individual steps</li> <li> <p>No undo/redo capability</p> </li> <li> <p>Tool Registry Limitations</p> </li> <li>Simple map lookup</li> <li>No tool versioning</li> <li>No tool dependencies</li> <li> <p>No tool validation</p> </li> <li> <p>Message Processing Issues</p> </li> <li>Processors hardcoded in <code>buildMessagesForCall</code></li> <li>No dynamic processor selection</li> <li> <p>Processors can't access full context easily</p> </li> <li> <p>No Tool Call Validation</p> </li> <li>Doesn't validate args against schema</li> <li>No type checking</li> <li> <p>Errors only surface at execution</p> </li> <li> <p>Limited Error Context</p> </li> <li>Errors don't include step number</li> <li>No error categorization</li> <li> <p>Hard to debug failures</p> </li> <li> <p>No Cancellation Support</p> <ul> <li>Can't cancel mid-execution</li> <li>No timeout per step</li> <li>Context passed but not checked</li> </ul> </li> </ol>"},{"location":"ENGINE_ARCHITECTURE/#whats-missing","title":"What's Missing","text":""},{"location":"ENGINE_ARCHITECTURE/#critical-missing-features","title":"Critical Missing Features","text":"<ol> <li> <p>Streaming Support <pre><code>// Current: Placeholder\nfunc Stream(...) (&lt;-chan StreamEvent, &lt;-chan error)\n\n// Needed: Real implementation\n- Incremental text output\n- Tool call streaming\n- Usage updates during generation\n</code></pre></p> </li> <li> <p>Token Counting <pre><code>// Current: estimate function placeholder\nfunc estimate([]ChatMessage) int\n\n// Needed: Real tokenizer\n- Provider-specific tokenizers\n- Accurate counting\n- Budget enforcement\n</code></pre></p> </li> <li> <p>Error Recovery <pre><code>// Needed:\n- Retry logic for transient errors\n- Tool error handling strategies\n- Graceful degradation\n</code></pre></p> </li> <li> <p>Tool Validation <pre><code>// Needed:\n- Validate args against JSON schema\n- Type checking\n- Required field validation\n</code></pre></p> </li> <li> <p>Cancellation &amp; Timeouts <pre><code>// Needed:\n- Per-step timeouts\n- Cancellation support\n- Context propagation\n</code></pre></p> </li> </ol>"},{"location":"ENGINE_ARCHITECTURE/#nice-to-have-features","title":"Nice-to-Have Features","text":"<ol> <li>Tool Dependencies</li> <li>Tools that depend on other tools</li> <li>Execution ordering</li> <li> <p>Dependency graph</p> </li> <li> <p>Tool Versioning</p> </li> <li>Multiple versions of same tool</li> <li>A/B testing</li> <li> <p>Gradual rollout</p> </li> <li> <p>State Snapshots</p> </li> <li>Save/restore state</li> <li>Checkpointing</li> <li> <p>Resume from failure</p> </li> <li> <p>Advanced Processors</p> </li> <li>Context-aware summarization</li> <li>Semantic compression</li> <li> <p>Relevance filtering</p> </li> <li> <p>Metrics &amp; Observability</p> </li> <li>Built-in metrics</li> <li>Performance tracking</li> <li> <p>Cost estimation</p> </li> <li> <p>Tool Middleware</p> </li> <li>Pre/post execution hooks</li> <li>Rate limiting</li> <li>Caching</li> </ol>"},{"location":"ENGINE_ARCHITECTURE/#current-bug-empty-tool-content","title":"Current Bug: Empty Tool Content","text":"<p>Symptom: <code>Invalid value for 'content': expected a string, got null</code></p> <p>Cause: When a tool returns an empty string, OpenAI SDK may serialize it as <code>null</code> instead of <code>\"\"</code>.</p> <p>Fix Needed: In <code>providers/openai.go</code>, ensure tool messages always have non-empty content:</p> <pre><code>case \"tool\":\n    content := msg.Content\n    if content == \"\" {\n        content = \"{}\"  // Empty JSON object instead of empty string\n    }\n    openaiMsgs = append(openaiMsgs, openai.ChatCompletionMessage{\n        Role:    openai.ChatMessageRoleTool,\n        Name:    msg.Name,\n        Content: content,\n    })\n</code></pre>"},{"location":"ENGINE_ARCHITECTURE/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"ENGINE_ARCHITECTURE/#issue-1-invalid-value-for-content-expected-a-string-got-null","title":"Issue 1: \"Invalid value for 'content': expected a string, got null\"","text":"<p>Cause: Tool result is empty or nil, provider expects string.</p> <p>Solution: Ensure all tools return non-empty strings: <pre><code>Fn: func(ctx, args) (string, error) {\n    result := doSomething()\n    if result == \"\" {\n        return \"{}\", nil  // Return empty JSON object, not empty string\n    }\n    return result, nil\n}\n</code></pre></p>"},{"location":"ENGINE_ARCHITECTURE/#issue-2-history-growing-too-large","title":"Issue 2: History Growing Too Large","text":"<p>Cause: No processors applied or insufficient compression.</p> <p>Solution: Use processors: <pre><code>msgs, err := ApplyProcessors(ctx, st, msgs,\n    SummarizeOlderThanN(llm, 10),\n    KeepLastN(8),\n    TruncateLongTools(2000),\n)\n</code></pre></p>"},{"location":"ENGINE_ARCHITECTURE/#issue-3-tools-not-executing-in-parallel","title":"Issue 3: Tools Not Executing in Parallel","text":"<p>Cause: Already parallel, but check for blocking operations.</p> <p>Solution: Ensure tools don't block each other: - Use separate goroutines for I/O - Don't share mutable state - Use channels for coordination if needed</p>"},{"location":"ENGINE_ARCHITECTURE/#issue-4-phase-detection-wrong","title":"Issue 4: Phase Detection Wrong","text":"<p>Cause: Naive detection based only on tool names.</p> <p>Solution: Improve <code>DetectPhase()</code>: - Look at tool sequences - Consider context - Use ML/pattern matching</p>"},{"location":"ENGINE_ARCHITECTURE/#extension-points","title":"Extension Points","text":""},{"location":"ENGINE_ARCHITECTURE/#adding-a-new-provider","title":"Adding a New Provider","text":"<ol> <li> <p>Implement <code>LLMClient</code> interface: <pre><code>type MyProvider struct { ... }\n\nfunc (p *MyProvider) Chat(...) (LLMResponse, error) {\n    // Convert engine.ChatMessage \u2192 provider format\n    // Call provider SDK\n    // Convert response \u2192 engine.LLMResponse\n    return resp, nil\n}\n</code></pre></p> </li> <li> <p>Add to factory: <pre><code>case \"myprovider\":\n    return NewMyProviderClient(...)\n</code></pre></p> </li> </ol>"},{"location":"ENGINE_ARCHITECTURE/#adding-a-new-processor","title":"Adding a New Processor","text":"<pre><code>func MyProcessor(param int) Processor {\n    return func(ctx, st *State, msgs []ChatMessage) ([]ChatMessage, error) {\n        // Transform messages\n        return transformed, nil\n    }\n}\n</code></pre>"},{"location":"ENGINE_ARCHITECTURE/#adding-a-new-hook","title":"Adding a New Hook","text":"<pre><code>type MyHook struct { ... }\n\nfunc (h MyHook) OnStepStart(ctx, st *State) {\n    // Your logic\n}\n\n// Implement other methods or use NopHook\n</code></pre>"},{"location":"ENGINE_ARCHITECTURE/#testing-strategy","title":"Testing Strategy","text":""},{"location":"ENGINE_ARCHITECTURE/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test <code>stepOnce()</code> with mocked LLM</li> <li>Test processors independently</li> <li>Test tool execution</li> <li>Test phase detection</li> </ul>"},{"location":"ENGINE_ARCHITECTURE/#integration-tests","title":"Integration Tests","text":"<ul> <li>Test full <code>Run()</code> loop</li> <li>Test with real providers (sandboxed)</li> <li>Test error scenarios</li> <li>Test parallel execution</li> </ul>"},{"location":"ENGINE_ARCHITECTURE/#performance-tests","title":"Performance Tests","text":"<ul> <li>Measure tool parallelization speedup</li> <li>Test with large histories</li> <li>Test processor performance</li> <li>Test memory usage</li> </ul>"},{"location":"ENGINE_ARCHITECTURE/#future-improvements","title":"Future Improvements","text":"<ol> <li>Streaming First</li> <li>Implement real streaming</li> <li>Incremental updates</li> <li> <p>Better UX</p> </li> <li> <p>Better State Management</p> </li> <li>Immutable state</li> <li>State snapshots</li> <li> <p>Undo/redo</p> </li> <li> <p>Smarter Processors</p> </li> <li>ML-based summarization</li> <li>Relevance scoring</li> <li> <p>Context-aware compression</p> </li> <li> <p>Tool System Enhancements</p> </li> <li>Tool dependencies</li> <li>Tool versioning</li> <li> <p>Tool middleware</p> </li> <li> <p>Error Handling</p> </li> <li>Retry strategies</li> <li>Error recovery</li> <li> <p>Better error messages</p> </li> <li> <p>Observability</p> </li> <li>Built-in metrics</li> <li>Tracing support</li> <li>Performance profiling</li> </ol>"},{"location":"ENGINE_ARCHITECTURE/#summary","title":"Summary","text":"<p>The engine is a simple, focused ReAct implementation that: - \u2705 Works with any LLM provider - \u2705 Executes tools in parallel - \u2705 Provides extensibility via hooks - \u2705 Handles long conversations via processors</p> <p>But it's missing: - \u274c Real streaming support - \u274c Token counting - \u274c Error recovery - \u274c Tool validation - \u274c Advanced state management</p> <p>The architecture is clean and extensible, making it easy to add these features incrementally.</p>"},{"location":"INK_UI/","title":"Ink-based CLI UI","text":"<p>The <code>ink-ui/</code> workspace contains a React + Ink client that talks to <code>dodo engine --stdio</code> using the NDJSON protocol defined in <code>docs/PROTO.md</code>.</p>"},{"location":"INK_UI/#prerequisites","title":"Prerequisites","text":"<ol> <li>Build or have access to the Go engine binary in the repo root:    <pre><code>go build -o dodo ./cmd/repl\n</code></pre>    (If the binary is missing, the UI falls back to <code>go run ./cmd/repl engine --stdio</code> automatically; building upfront avoids repeated compilation.)</li> <li>Install the Ink UI dependencies once:    <pre><code>cd ink-ui\nnpm install\n</code></pre></li> </ol>"},{"location":"INK_UI/#running-the-ui","title":"Running the UI","text":"<p>From the <code>ink-ui/</code> directory: <pre><code>npm run dev -- --repo /path/to/target/repo\n</code></pre></p> <p>Optional flags:</p> <ul> <li><code>--session-id &lt;id&gt;</code> \u2013 ask the engine to reuse/label the session with a specific id.</li> <li><code>--engine &lt;path&gt;</code> \u2013 path to a prebuilt <code>dodo</code> binary (defaults to <code>../dodo</code> if present, otherwise <code>go run ./cmd/repl</code>).</li> <li><code>--engine-cwd &lt;path&gt;</code> \u2013 working directory for the engine command (defaults to the repository root).</li> </ul> <p>The UI automatically sends a <code>start_session</code> command on boot and begins streaming events as soon as the engine emits <code>status=session_ready</code>.</p>"},{"location":"INK_UI/#layout-overview","title":"Layout Overview","text":"<ul> <li>Header \u2013 Displays the repo name, current session id (shortened), overall status, and the engine command that was launched.</li> <li>Main pane \u2013 Shows the conversation history. User prompts appear in green; streaming assistant responses append in place. Summaries emitted via <code>respond.summary</code> are rendered beneath the assistant text.</li> <li>Side pane \u2013 Lists the most recent <code>tool_event</code> messages and the latest <code>files_changed</code> payload.</li> <li>Footer \u2013 Input box for the next user instruction plus a placeholder token counter. Input is disabled while a request is active or before the session becomes ready.</li> </ul>"},{"location":"INK_UI/#protocol-mapping","title":"Protocol Mapping","text":"<p>The <code>EngineClient</code> class mirrors the Go protocol types (see <code>internal/engine/protocol</code>):</p> <ul> <li>Commands sent upstream:</li> <li><code>start_session</code> (automatic on boot)</li> <li><code>user_message</code> (on Enter)</li> <li>Events handled downstream:</li> <li><code>status</code> \u2192 drives the status chip + info banner.</li> <li><code>assistant_text</code> \u2192 populates the streaming area (<code>delta</code>, <code>assistant</code>, <code>respond.summary</code>).</li> <li><code>tool_event</code> / <code>files_changed</code> \u2192 update the side pane.</li> <li><code>done</code> \u2192 marks the turn complete, re-enables input, caches summary + files.</li> <li><code>error</code> \u2192 shows an inline error banner and clears the pending state.</li> </ul> <p>Refer to <code>docs/PROTO.md</code> for the canonical NDJSON contract.</p>"},{"location":"INK_UI/#manual-test-checklist","title":"Manual Test Checklist","text":"<ol> <li>Start the UI against the repo itself:    <pre><code>cd ink-ui\nnpm run dev -- --repo ..\n</code></pre></li> <li>When the header shows <code>session_ready</code>, type a request (e.g., \u201cSummarize main.go\u201d).</li> <li>Observe:</li> <li>Streaming assistant output in the main pane.</li> <li>Tool invocations appearing in the sidebar.</li> <li><code>files_changed</code> list updating after completion (if the agent reports file edits).</li> <li>Send another request to confirm multi-turn behavior.</li> <li>Exit with <code>Ctrl+C</code>; the UI forwards the signal to the engine and shuts down cleanly.</li> </ol>"},{"location":"OPTIMIZATION_GUIDE/","title":"\ud83d\ude80 Complete Optimization Guide","text":""},{"location":"OPTIMIZATION_GUIDE/#current-performance-kimi-k2","title":"\ud83d\udcca Current Performance (Kimi K2)","text":""},{"location":"OPTIMIZATION_GUIDE/#tps-tokens-per-second-analysis","title":"TPS (Tokens Per Second) Analysis","text":"<p>From your full run (8m 17s): - LLM Generation Time: 206 seconds - Estimated Output Tokens: ~5,500-6,000 tokens   - renderer.go: ~1,500-2,000 tokens (220 lines)   - input.go: ~600-800 tokens (86 lines)   - index.html: ~400-500 tokens (39 lines)   - game.js: ~2,000-2,500 tokens (182 lines)   - styles.css: ~500-600 tokens (167 lines)   - Plus tool calls, reasoning, responses: ~500-1,000 tokens</p> <p>Average TPS: ~27-29 tokens/second (output tokens)</p> <p>Range observed: 25-80 tokens/second (varies by complexity) - Simple files (HTML/CSS): 25-30 TPS - Medium files (Go): 40-50 TPS - Complex files (JS with logic): 35-45 TPS</p>"},{"location":"OPTIMIZATION_GUIDE/#comparison-with-other-models","title":"Comparison with Other Models","text":"Model Output TPS Input TPS Cost (1M out) Best For Kimi K2 25-80 ~500+ $2.50 Budget tasks GPT-4o 150-200 ~1000+ $10.00 Multi-tool, complex GPT-4o-mini 200-300 ~2000+ $0.60 Simple files, fast Claude 3.5 Sonnet 100-150 ~800+ $15.00 Reasoning, quality <p>Kimi K2 is 3-4x slower than GPT-4o for output, but 4x cheaper.</p>"},{"location":"OPTIMIZATION_GUIDE/#optimization-strategies-ranked-by-impact","title":"\ud83c\udfaf Optimization Strategies (Ranked by Impact)","text":""},{"location":"OPTIMIZATION_GUIDE/#strategy-1-parallel-file-creation-biggest-win","title":"\ud83e\udd47 Strategy 1: Parallel File Creation (BIGGEST WIN)","text":"<p>Current Problem: - Files created sequentially (5 separate LLM responses) - 156 seconds wasted on sequential generation - Each file waits for previous to complete</p> <p>Solution Options:</p>"},{"location":"OPTIMIZATION_GUIDE/#option-a-use-gpt-4o-for-multi-file-tasks-recommended","title":"Option A: Use GPT-4o for Multi-File Tasks (RECOMMENDED)","text":"<pre><code># For multi-file creation tasks\nexport LLM_PROVIDER=openai\nexport OPENAI_MODEL=gpt-4o\n./dodo --repo ../gosnake --task \"add browser renderer\"\n</code></pre> <p>Expected: - All 5 files in ONE response (~20s generation) - 5 tool calls executed in parallel (~5ms) - Savings: 136 seconds (87% faster)</p> <p>Cost Impact: - Kimi K2: ~$0.015 (6K tokens \u00d7 $2.50/1M) - GPT-4o: ~$0.06 (6K tokens \u00d7 $10/1M) - 4x more expensive, but 7x faster</p>"},{"location":"OPTIMIZATION_GUIDE/#option-b-implement-batching-at-framework-level","title":"Option B: Implement Batching at Framework Level","text":"<p>If GPT-4o doesn't parallelize either, implement:</p> <pre><code>// In internal/tools/parallel_executor.go\n// Queue multiple write() calls and execute together\nfunc BatchWrite(files []WriteParams) []WriteResult {\n    // Execute all writes in parallel\n    // Return when all complete\n}\n</code></pre> <p>Savings: 136 seconds (if model supports it)</p>"},{"location":"OPTIMIZATION_GUIDE/#option-c-hybrid-approach","title":"Option C: Hybrid Approach","text":"<ul> <li>Planning phase: Use Kimi K2 (cheap, good reasoning)</li> <li>File creation: Use GPT-4o-mini (fast, cheap for simple files)</li> <li>Complex logic: Use GPT-4o (quality)</li> </ul> <p>Savings: 100-120 seconds</p>"},{"location":"OPTIMIZATION_GUIDE/#strategy-2-better-planning-second-biggest-win","title":"\ud83e\udd48 Strategy 2: Better Planning (SECOND BIGGEST WIN)","text":"<p>Current Problem: - 4 build failures (184 seconds wasted) - Missing files discovered during build - Incremental file creation</p> <p>Solution: Force Complete Planning in Step 1</p>"},{"location":"OPTIMIZATION_GUIDE/#implementation","title":"Implementation","text":"<pre><code>// In internal/agent/interactive.go\n// Add to UNDERSTAND_AND_REASON step:\n\n\"STEP 1: PLAN COMPLETELY\nBefore writing ANY code, you MUST:\n1. List ALL files that need to be created\n2. List ALL files that need to be modified\n3. Verify dependencies and imports\n4. Create a checklist\n\nUse the 'think' tool to create a complete plan:\n- File 1: path/to/file.go (purpose, dependencies)\n- File 2: path/to/file.js (purpose, dependencies)\n- ... (all files)\n\nONLY after the plan is complete, proceed to creation.\"\n</code></pre> <p>Expected: - 1-2 builds instead of 4-5 - All files created before first build - Savings: 154 seconds (84% faster)</p>"},{"location":"OPTIMIZATION_GUIDE/#add-pre-write-validation","title":"Add Pre-Write Validation","text":"<pre><code>// In internal/tools/write.go\nfunc ValidateBeforeWrite(path string, content string) error {\n    // Check imports exist\n    // Check file structure\n    // Check Go syntax (basic)\n    // Return errors before writing\n}\n</code></pre> <p>Savings: 30-50 seconds (catch errors early)</p>"},{"location":"OPTIMIZATION_GUIDE/#strategy-3-model-selection-strategy","title":"\ud83e\udd49 Strategy 3: Model Selection Strategy","text":"<p>Current: Always use Kimi K2</p> <p>Optimized: Smart model selection</p>"},{"location":"OPTIMIZATION_GUIDE/#rules","title":"Rules","text":"<pre><code>func SelectModel(task string, fileCount int) string {\n    // Multi-file creation (&gt;3 files)\n    if fileCount &gt; 3 {\n        return \"gpt-4o\" // Better parallel tool calls\n    }\n\n    // Simple files (HTML/CSS/JSON)\n    if isSimpleFile(task) {\n        return \"gpt-4o-mini\" // Fast, cheap\n    }\n\n    // Complex reasoning\n    if requiresDeepReasoning(task) {\n        return \"claude-3-5-sonnet\" // Best quality\n    }\n\n    // Default: Kimi K2 (budget)\n    return \"kimi-k2\"\n}\n</code></pre> <p>Expected Savings: - Simple files: 40s \u2192 15s (62% faster) - Multi-file: 156s \u2192 20s (87% faster)</p>"},{"location":"OPTIMIZATION_GUIDE/#4-strategy-4-template-system","title":"4\ufe0f\u20e3 Strategy 4: Template System","text":"<p>Current: LLM generates all boilerplate</p> <p>Optimized: Pre-generate templates, LLM fills logic</p>"},{"location":"OPTIMIZATION_GUIDE/#implementation_1","title":"Implementation","text":"<pre><code>// internal/tools/template.go\ntype Template struct {\n    Name        string\n    Boilerplate string\n    Variables   []string\n}\n\nvar templates = map[string]Template{\n    \"go-http-server\": {\n        Boilerplate: `package main\n\nimport (\n    \"net/http\"\n    \"fmt\"\n)\n\nfunc main() {\n    http.HandleFunc(\"/\", handler)\n    http.ListenAndServe(\":8080\", nil)\n}\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    // {{LOGIC}}\n}\n`,\n        Variables: []string{\"LOGIC\"},\n    },\n}\n</code></pre> <p>Savings: - Reduce tokens by 30-40% - Faster generation (less to generate) - ~60-80 seconds saved</p>"},{"location":"OPTIMIZATION_GUIDE/#5-strategy-5-streaming-responses","title":"5\ufe0f\u20e3 Strategy 5: Streaming Responses","text":"<p>Current: Wait for full response before executing tools</p> <p>Optimized: Execute tools as they're generated</p>"},{"location":"OPTIMIZATION_GUIDE/#implementation_2","title":"Implementation","text":"<pre><code>// In LLM response handler\n// Parse tool calls incrementally\n// Execute as soon as complete\n// Don't wait for full response\n</code></pre> <p>Savings: 10-20 seconds (overlap generation with execution)</p>"},{"location":"OPTIMIZATION_GUIDE/#6-strategy-6-caching-context-optimization","title":"6\ufe0f\u20e3 Strategy 6: Caching &amp; Context Optimization","text":"<p>Current: System prompt ~10K tokens (repeated every turn)</p> <p>Optimized:  - Cache system prompt (Anthropic supports this) - Reduce workspace context (only relevant files) - Use embeddings for context instead of full files</p> <p>Savings: - Cached tokens: 90% discount (Anthropic) - Reduced context: 30-40% fewer input tokens - ~$0.01-0.02 per task saved</p>"},{"location":"OPTIMIZATION_GUIDE/#7-strategy-7-incremental-builds","title":"7\ufe0f\u20e3 Strategy 7: Incremental Builds","text":"<p>Current: Full build after all files</p> <p>Optimized:  - Build after each file (catch errors early) - Or: Validate syntax before writing</p> <p>Savings: 30-50 seconds (catch errors faster)</p>"},{"location":"OPTIMIZATION_GUIDE/#expected-performance-after-all-optimizations","title":"\ud83d\udcca Expected Performance After All Optimizations","text":""},{"location":"OPTIMIZATION_GUIDE/#current-performance-kimi-k2_1","title":"Current Performance (Kimi K2)","text":"<pre><code>Setup:           10s\nUnderstanding:   12s\nFile Creation:   156s (sequential) \u274c\nBuild Fixes:     184s (4 failures) \u274c\nCompletion:      16s\n--------------------------------\nTotal:           497s (8m 17s)\nTPS:             ~27-29 tokens/second\nCost:            ~$0.015\n</code></pre>"},{"location":"OPTIMIZATION_GUIDE/#optimized-performance-gpt-4o-better-planning","title":"Optimized Performance (GPT-4o + Better Planning)","text":"<pre><code>Setup:           10s\nUnderstanding:   12s\nFile Creation:   20s (parallel) \u2705\nBuild Fixes:     30s (1-2 builds) \u2705\nCompletion:      16s\n--------------------------------\nTotal:           88s (1m 28s)\nTPS:             ~150-200 tokens/second\nCost:            ~$0.06\n</code></pre> <p>Improvement:  - Time: 497s \u2192 88s (82% faster, 5.6x speedup) - Cost: 4x more expensive ($0.015 \u2192 $0.06) - Value: Worth it for time savings (5.6x faster for 4x cost)</p>"},{"location":"OPTIMIZATION_GUIDE/#optimized-performance-hybrid-approach","title":"Optimized Performance (Hybrid Approach)","text":"<pre><code>Planning (Kimi):     15s\nFile Creation (GPT-4o-mini): 25s\nComplex Logic (GPT-4o):      30s\nBuild Fixes:         30s\nCompletion:         16s\n--------------------------------\nTotal:               116s (1m 56s)\nCost:                ~$0.03\n</code></pre> <p>Improvement: - Time: 497s \u2192 116s (77% faster, 4.3x speedup) - Cost: 2x more expensive ($0.015 \u2192 $0.03) - Best balance: Fast + affordable</p>"},{"location":"OPTIMIZATION_GUIDE/#implementation-priority","title":"\ud83c\udfaf Implementation Priority","text":""},{"location":"OPTIMIZATION_GUIDE/#phase-1-quick-wins-this-week","title":"Phase 1: Quick Wins (This Week)","text":"<ol> <li>\u2705 Test GPT-4o for multi-file tasks</li> <li>Time: 5 minutes</li> <li>Impact: 136 seconds saved</li> <li> <p>Cost: +$0.045 per task</p> </li> <li> <p>\u2705 Improve planning prompt</p> </li> <li>Time: 30 minutes</li> <li>Impact: 154 seconds saved</li> <li> <p>Cost: $0</p> </li> <li> <p>\u2705 Add pre-write validation</p> </li> <li>Time: 2 hours</li> <li>Impact: 30-50 seconds saved</li> <li>Cost: $0</li> </ol> <p>Total Phase 1 Savings: 320 seconds (64% faster)</p>"},{"location":"OPTIMIZATION_GUIDE/#phase-2-medium-term-this-month","title":"Phase 2: Medium Term (This Month)","text":"<ol> <li>Model selection strategy</li> <li>Time: 1 day</li> <li>Impact: 40-60 seconds per simple file</li> <li> <p>Cost: Variable (but optimized)</p> </li> <li> <p>Template system</p> </li> <li>Time: 2-3 days</li> <li>Impact: 60-80 seconds saved</li> <li>Cost: $0</li> </ol> <p>Total Phase 2 Savings: 100-140 seconds additional</p>"},{"location":"OPTIMIZATION_GUIDE/#phase-3-advanced-next-month","title":"Phase 3: Advanced (Next Month)","text":"<ol> <li>Streaming responses</li> <li>Time: 3-5 days</li> <li>Impact: 10-20 seconds saved</li> <li> <p>Cost: $0</p> </li> <li> <p>Context optimization</p> </li> <li>Time: 2-3 days</li> <li>Impact: Cost savings, faster input</li> <li>Cost: -$0.01-0.02 per task</li> </ol>"},{"location":"OPTIMIZATION_GUIDE/#quick-reference-when-to-use-which-model","title":"\ud83d\udca1 Quick Reference: When to Use Which Model","text":""},{"location":"OPTIMIZATION_GUIDE/#use-kimi-k2-when","title":"Use Kimi K2 When:","text":"<ul> <li>\u2705 Budget is primary concern</li> <li>\u2705 Single file edits</li> <li>\u2705 Simple tasks</li> <li>\u2705 Can tolerate slower speed</li> </ul>"},{"location":"OPTIMIZATION_GUIDE/#use-gpt-4o-when","title":"Use GPT-4o When:","text":"<ul> <li>\u2705 Multi-file creation (&gt;3 files)</li> <li>\u2705 Need parallel tool calls</li> <li>\u2705 Complex architecture changes</li> <li>\u2705 Speed is critical</li> </ul>"},{"location":"OPTIMIZATION_GUIDE/#use-gpt-4o-mini-when","title":"Use GPT-4o-mini When:","text":"<ul> <li>\u2705 Simple file creation (HTML/CSS/JSON)</li> <li>\u2705 Fast iteration needed</li> <li>\u2705 Low cost + speed balance</li> </ul>"},{"location":"OPTIMIZATION_GUIDE/#use-claude-35-sonnet-when","title":"Use Claude 3.5 Sonnet When:","text":"<ul> <li>\u2705 Complex reasoning required</li> <li>\u2705 Quality is critical</li> <li>\u2705 Need best code quality</li> </ul>"},{"location":"OPTIMIZATION_GUIDE/#tps-benchmarks-by-model","title":"\ud83d\udcc8 TPS Benchmarks by Model","text":""},{"location":"OPTIMIZATION_GUIDE/#output-tps-tokens-per-second","title":"Output TPS (Tokens Per Second)","text":"Model Min Avg Max Use Case Kimi K2 25 27-29 80 Budget, simple GPT-4o 150 175 200 Multi-tool, complex GPT-4o-mini 200 250 300 Simple files, fast Claude 3.5 Sonnet 100 125 150 Quality, reasoning"},{"location":"OPTIMIZATION_GUIDE/#input-tps-context-processing","title":"Input TPS (Context Processing)","text":"Model TPS Notes Kimi K2 500+ Fast context processing GPT-4o 1000+ Very fast GPT-4o-mini 2000+ Extremely fast Claude 3.5 Sonnet 800+ Fast with caching"},{"location":"OPTIMIZATION_GUIDE/#key-takeaways","title":"\ud83c\udf93 Key Takeaways","text":"<ol> <li>Your average TPS: ~27-29 tokens/second (Kimi K2 output)</li> <li>Biggest bottleneck: Sequential file creation (156s wasted)</li> <li>Second bottleneck: Build failures (184s wasted)</li> <li>Write tool is fast: 0.76-1.96ms per file (NOT the problem)</li> <li>Best optimization: Use GPT-4o for multi-file tasks + better planning</li> <li>Expected improvement: 497s \u2192 88s (82% faster)</li> </ol>"},{"location":"OPTIMIZATION_GUIDE/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ol> <li> <p>Test GPT-4o now (5 minutes):    <pre><code>export LLM_PROVIDER=openai\nexport OPENAI_MODEL=gpt-4o\n./dodo --repo ../gosnake --task \"add browser renderer\"\n</code></pre></p> </li> <li> <p>Improve planning prompt (30 minutes):</p> </li> <li>Add \"list all files\" requirement</li> <li> <p>Force complete planning before creation</p> </li> <li> <p>Monitor results:</p> </li> <li>Track TPS improvement</li> <li>Measure time savings</li> <li>Compare costs</li> </ol> <p>Expected result: 8m 17s \u2192 1m 28s (82% faster) \ud83c\udfaf</p>"},{"location":"PROTO/","title":"PROTO","text":""},{"location":"PROTO/#engine-cli-ndjson-protocol","title":"Engine \u2194 CLI NDJSON Protocol","text":"<p>This document captures the minimal contract used by <code>dodo engine --stdio</code>. Each line written to stdin/stdout is a standalone JSON object with a <code>type</code> field.</p>"},{"location":"PROTO/#commands-cli-engine","title":"Commands (CLI \u279c Engine)","text":"Type JSON shape Notes <code>start_session</code> <code>{\"type\":\"start_session\",\"session_id\":\"optional\",\"repo_root\":\"/path\",\"meta\":{...}}</code> If <code>session_id</code> is omitted the engine generates one. When this command succeeds, the first event referencing the session is <code>status=session_ready</code>, which contains the canonical <code>session_id</code>. Treat that event as the authoritative ID even if the client proposed another value. <code>user_message</code> <code>{\"type\":\"user_message\",\"session_id\":\"abc123\",\"message\":\"...\"}</code> Adds a user turn to an existing session."},{"location":"PROTO/#events-engine-cli","title":"Events (Engine \u279c CLI)","text":"<p>All events include the session id when they relate to a specific session (<code>\"session_id\":\"abc123\"</code>). Global events (e.g. engine startup) omit it.</p> Type Fields Semantics <code>status</code> <code>status</code>, <code>detail</code> <code>status=\"engine_ready\"</code> is emitted once when the stdio bridge is ready to accept commands. <code>status=\"session_ready\"</code> is emitted after a successful <code>start_session</code> and signals that the engine recognized the session; clients should treat this event as the session acknowledgment. Other statuses (<code>thinking</code>, <code>step_start</code>, <code>retry</code>, <code>budget_exceeded</code>, <code>done</code>, etc.) track lifecycle progress. <code>assistant_text</code> <code>content</code>, <code>source</code>, <code>final?</code> The <code>source</code> field indicates how the text should be presented: <code>delta</code> = incremental streaming tokens, <code>assistant</code> = non-streaming assistant response, <code>respond.summary</code> = structured summary returned by the <code>respond</code> tool (safe to show in a summary pane). <code>tool_event</code> <code>tool</code>, <code>phase</code>, <code>success?</code>, <code>details?</code> Tool lifecycle notifications (<code>phase=\"start\"</code>/<code>\"end\"</code>). <code>files_changed</code> <code>files[]</code> Emitted when the agent reports file modifications (typically sourced from the <code>respond</code> tool payload). <code>done</code> <code>summary</code>, <code>files_changed[]</code> Session request completed. <code>error</code> <code>message</code>, <code>kind</code>, <code>details?</code> Protocol or engine errors that the client should surface. <p>Future transports (Ink UI, IDE integration, WebSocket) should reuse these structures for consistency.</p>"},{"location":"TESTING_GUIDE/","title":"\ud83e\uddea Testing Guide for Semantic Search Tools","text":"<p>This guide helps you validate that the <code>codebase_search</code> and <code>read_span</code> tools are working correctly.</p>"},{"location":"TESTING_GUIDE/#quick-start-manual-testing","title":"\ud83d\ude80 Quick Start: Manual Testing","text":""},{"location":"TESTING_GUIDE/#step-1-choose-a-test-repository","title":"Step 1: Choose a Test Repository","text":"<p>Pick a repository you know well (preferably one with Go/Python code):</p> <pre><code>cd /path/to/your/test-repo\n</code></pre>"},{"location":"TESTING_GUIDE/#step-2-initial-indexing-one-time","title":"Step 2: Initial Indexing (One Time)","text":"<pre><code># Set OpenAI API key if you want semantic search\nexport OPENAI_API_KEY=\"your-key-here\"\n\n# Run initial indexing\ndodo --repo . --task \"test\" --index\n</code></pre> <p>What to expect: - \u2705 Logs showing file discovery - \u2705 \"Starting initial full indexing...\" - \u2705 Progress updates as files are chunked and embedded - \u2705 \"BM25 index ready\" - \u2705 Database created at <code>.dodo/index.db</code> - \u2705 BM25 index created at <code>.dodo/index.db.bleve</code></p> <p>Time estimate: - Small repo (&lt; 100 files): ~30 seconds - Medium repo (1k files): ~2-5 minutes - Large repo (10k+ files): ~10-30 minutes</p>"},{"location":"TESTING_GUIDE/#step-3-test-with-a-real-query","title":"Step 3: Test with a Real Query","text":"<pre><code># Ask a question about your codebase\ndodo --repo . --task \"Find where we handle authentication\"\n</code></pre> <p>What to expect: - \u2705 Agent uses <code>codebase_search</code> tool - \u2705 Returns relevant code locations - \u2705 Agent uses <code>read_span</code> to read full context - \u2705 Agent provides answer based on found code</p>"},{"location":"TESTING_GUIDE/#validation-checklist","title":"\ud83d\udd0d Validation Checklist","text":""},{"location":"TESTING_GUIDE/#indexing-validation","title":"\u2705 Indexing Validation","text":"<ol> <li> <p>Check database exists: <pre><code>ls -lh .dodo/index.db\n</code></pre></p> </li> <li> <p>Check BM25 index exists: <pre><code>ls -lh .dodo/index.db.bleve/\n</code></pre></p> </li> <li> <p>Verify files are indexed: <pre><code>sqlite3 .dodo/index.db \"SELECT COUNT(*) FROM files WHERE index_status = 'indexed';\"\n</code></pre></p> </li> <li> <p>Verify chunks are created: <pre><code>sqlite3 .dodo/index.db \"SELECT COUNT(*) FROM chunks;\"\n</code></pre></p> </li> <li> <p>Verify embeddings exist (if OpenAI key set): <pre><code>sqlite3 .dodo/index.db \"SELECT COUNT(*) FROM embeddings;\"\n</code></pre></p> </li> </ol>"},{"location":"TESTING_GUIDE/#tool-registration-validation","title":"\u2705 Tool Registration Validation","text":"<ol> <li>Check agent logs for tool registration:</li> <li>Look for logs showing tools being registered</li> <li> <p>Should see <code>codebase_search</code> and <code>read_span</code> in the tool list</p> </li> <li> <p>Verify system prompt includes search guidance:</p> </li> <li>Check logs for: \"To find relevant code or documentation, use the 'codebase_search' tool...\"</li> </ol>"},{"location":"TESTING_GUIDE/#search-functionality-validation","title":"\u2705 Search Functionality Validation","text":"<p>Test these scenarios:</p>"},{"location":"TESTING_GUIDE/#test-1-keyword-search-bm25","title":"Test 1: Keyword Search (BM25)","text":"<p><pre><code>dodo --repo . --task \"Find functions that use JWT\"\n</code></pre> Expected: Should find code with \"JWT\" keyword even if query doesn't match semantically.</p>"},{"location":"TESTING_GUIDE/#test-2-semantic-search-embeddings","title":"Test 2: Semantic Search (Embeddings)","text":"<p><pre><code>dodo --repo . --task \"Where do we validate user credentials\"\n</code></pre> Expected: Should find authentication/validation code even if exact keywords don't match.</p>"},{"location":"TESTING_GUIDE/#test-3-hybrid-search-both","title":"Test 3: Hybrid Search (Both)","text":"<p><pre><code>dodo --repo . --task \"Show me error handling patterns\"\n</code></pre> Expected: Should combine keyword matches (BM25) with semantic matches (embeddings).</p>"},{"location":"TESTING_GUIDE/#test-4-file-filtering","title":"Test 4: File Filtering","text":"<p><pre><code>dodo --repo . --task \"Find test files that use mocks\"\n</code></pre> Expected: Should filter to test files (if globs are used).</p>"},{"location":"TESTING_GUIDE/#advanced-testing","title":"\ud83d\udee0\ufe0f Advanced Testing","text":""},{"location":"TESTING_GUIDE/#test-with-debug-logging","title":"Test with Debug Logging","text":"<p>Add verbose logging to see what's happening:</p> <pre><code># Enable Go debug logging\nexport GODEBUG=gctrace=1\n\n# Run with verbose output\ndodo --repo . --task \"test\" 2&gt;&amp;1 | tee dodo.log\n</code></pre>"},{"location":"TESTING_GUIDE/#inspect-the-index","title":"Inspect the Index","text":"<pre><code># Check indexed files\nsqlite3 .dodo/index.db \"SELECT path, lang, index_status FROM files LIMIT 10;\"\n\n# Check chunks\nsqlite3 .dodo/index.db \"SELECT file_path, start_line, end_line, kind FROM chunks LIMIT 10;\"\n\n# Check symbols\nsqlite3 .dodo/index.db \"SELECT name, kind, file_path FROM symbols LIMIT 10;\"\n</code></pre>"},{"location":"TESTING_GUIDE/#test-readspan-directly","title":"Test ReadSpan Directly","text":"<p>Create a simple test script:</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"github.com/yourorg/dodo/internal/indexer\"\n)\n\nfunc main() {\n    // Load manager (same setup as main.go)\n    manager := setupManager()\n\n    // Test ReadSpan\n    ctx := context.Background()\n    code, err := manager.ReadSpan(ctx, \"internal/indexer/manager.go\", 100, 120)\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Println(code)\n}\n</code></pre>"},{"location":"TESTING_GUIDE/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"TESTING_GUIDE/#problem-no-results-found","title":"Problem: \"No results found\"","text":"<p>Possible causes: 1. Indexing didn't complete 2. Files not indexed yet (check <code>index_status</code>) 3. Query too specific or too vague</p> <p>Solutions: - Check indexing status: <code>sqlite3 .dodo/index.db \"SELECT COUNT(*) FROM chunks;\"</code> - Re-run indexing: <code>dodo --repo . --task \"test\" --index</code> - Try simpler queries first</p>"},{"location":"TESTING_GUIDE/#problem-bm25-search-failed","title":"Problem: \"BM25 search failed\"","text":"<p>Possible causes: 1. BM25 index not created 2. Index corrupted</p> <p>Solutions: - Check BM25 index exists: <code>ls .dodo/index.db.bleve/</code> - Delete and re-index: <code>rm -rf .dodo/ &amp;&amp; dodo --repo . --task \"test\" --index</code></p>"},{"location":"TESTING_GUIDE/#problem-embedding-search-failed","title":"Problem: \"Embedding search failed\"","text":"<p>Possible causes: 1. No OpenAI API key 2. API key invalid 3. Network issues</p> <p>Solutions: - Check API key: <code>echo $OPENAI_API_KEY</code> - Test API key: <code>curl https://api.openai.com/v1/models -H \"Authorization: Bearer $OPENAI_API_KEY\"</code> - System will fall back to BM25-only if embeddings fail</p>"},{"location":"TESTING_GUIDE/#problem-tools-not-registered","title":"Problem: Tools not registered","text":"<p>Possible causes: 1. Manager not passed to workflow 2. Retrieval interface not implemented</p> <p>Solutions: - Check <code>workflow.go</code> passes manager - Check <code>agent.go</code> receives retrieval - Check logs for tool registration</p>"},{"location":"TESTING_GUIDE/#expected-results","title":"\ud83d\udcca Expected Results","text":""},{"location":"TESTING_GUIDE/#good-results","title":"Good Results \u2705","text":"<ul> <li>Fast search: &lt; 1 second for most queries</li> <li>Relevant results: Top 3 results should be highly relevant</li> <li>Hybrid working: Results show <code>\"reason\": \"rrf(bm25+vec)\"</code></li> <li>Agent uses tools: Logs show <code>codebase_search</code> and <code>read_span</code> calls</li> </ul>"},{"location":"TESTING_GUIDE/#warning-signs","title":"Warning Signs \u26a0\ufe0f","text":"<ul> <li>Slow search: &gt; 2 seconds (might need optimization)</li> <li>Irrelevant results: Top results don't match query</li> <li>Only BM25: Results show <code>\"reason\": \"embedding_only\"</code> (embeddings not working)</li> <li>No results: Empty array returned (index might be empty)</li> </ul>"},{"location":"TESTING_GUIDE/#test-scenarios","title":"\ud83c\udfaf Test Scenarios","text":""},{"location":"TESTING_GUIDE/#scenario-1-find-a-specific-function","title":"Scenario 1: Find a Specific Function","text":"<pre><code>dodo --repo . --task \"Find the function that validates JWT tokens\"\n</code></pre> <p>Expected behavior: 1. Agent calls <code>codebase_search</code> with query 2. Returns function locations 3. Agent calls <code>read_span</code> on most relevant result 4. Agent provides answer with function name and location</p>"},{"location":"TESTING_GUIDE/#scenario-2-understand-code-pattern","title":"Scenario 2: Understand Code Pattern","text":"<pre><code>dodo --repo . --task \"How do we handle database errors in this codebase\"\n</code></pre> <p>Expected behavior: 1. Agent searches for error handling patterns 2. Returns multiple relevant locations 3. Agent reads multiple spans to understand pattern 4. Agent provides comprehensive answer</p>"},{"location":"TESTING_GUIDE/#scenario-3-find-test-files","title":"Scenario 3: Find Test Files","text":"<pre><code>dodo --repo . --task \"Show me test files for the authentication module\"\n</code></pre> <p>Expected behavior: 1. Agent searches with file filtering (if implemented) 2. Returns test files matching query 3. Agent reads relevant test code 4. Agent lists test files and their purpose</p>"},{"location":"TESTING_GUIDE/#unit-testing-for-developers","title":"\ud83d\udd2c Unit Testing (For Developers)","text":"<p>See <code>internal/indexer/</code> for unit test examples:</p> <pre><code># Run all tests\ngo test ./internal/indexer/...\n\n# Run specific test\ngo test ./internal/indexer/ -run TestSearch\n\n# With coverage\ngo test ./internal/indexer/ -cover\n</code></pre>"},{"location":"TESTING_GUIDE/#next-steps","title":"\ud83d\udcdd Next Steps","text":"<p>Once basic testing passes:</p> <ol> <li>Test on larger repos (10k+ files)</li> <li>Test with different languages (Go, Python, TypeScript, etc.)</li> <li>Test edge cases (empty repos, very large files, binary files)</li> <li>Measure performance (indexing time, search latency)</li> <li>Gather feedback (are results relevant? is search fast enough?)</li> </ol>"},{"location":"TESTING_GUIDE/#tips","title":"\ud83d\udca1 Tips","text":"<ol> <li>Start small: Test on a small, familiar repo first</li> <li>Use known queries: Ask about code you know exists</li> <li>Check logs: Watch for errors or warnings</li> <li>Be patient: First indexing takes time</li> <li>Iterate: Try different queries to understand behavior</li> </ol> <p>Happy testing! \ud83c\udf89</p>"},{"location":"TUI_GUIDE/","title":"Dodo Interactive TUI Mode","text":""},{"location":"TUI_GUIDE/#overview","title":"Overview","text":"<p>Dodo now features an interactive Terminal User Interface (TUI) powered by Bubble Tea, allowing you to run multiple tasks in a single session with real-time metrics and progress tracking.</p>"},{"location":"TUI_GUIDE/#features","title":"Features","text":"<p>\u2728 Interactive Session - Ask multiple tasks without restarting \ud83d\udcca Live Metrics - Real-time token usage, steps, and cost tracking \ud83c\udfa8 Beautiful UI - Clean, colorful interface with animations \ud83d\udcdd Task History - View previous tasks and results \u26a1 Fast Workflow - No need to restart for each task</p>"},{"location":"TUI_GUIDE/#usage","title":"Usage","text":""},{"location":"TUI_GUIDE/#start-tui-mode","title":"Start TUI Mode","text":"<pre><code>./dodo --repo /path/to/repo --tui\n</code></pre>"},{"location":"TUI_GUIDE/#with-indexing","title":"With Indexing","text":"<pre><code>./dodo --repo /path/to/repo --tui --index\n</code></pre>"},{"location":"TUI_GUIDE/#with-model-override","title":"With Model Override","text":"<pre><code>./dodo --repo /path/to/repo --tui --model gpt-4o\n</code></pre>"},{"location":"TUI_GUIDE/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<ul> <li>Enter: Submit task</li> <li>Ctrl+U: Clear input</li> <li>Ctrl+C (idle): Quit</li> <li>Ctrl+C (running): Cancel current task</li> <li>Q (idle): Quit</li> </ul>"},{"location":"TUI_GUIDE/#interface-layout","title":"Interface Layout","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83e\udda4 DODO Interactive Agent          Tasks: 3             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                          \u2502\n\u2502  \u2705 Task Completed                                       \u2502\n\u2502                                                          \u2502\n\u2502  Task: Add borders to snake game                        \u2502\n\u2502                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 I've successfully implemented borders...           \u2502 \u2502\n\u2502  \u2502 - Updated Renderer interface                       \u2502 \u2502\n\u2502  \u2502 - Implemented border drawing                       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 \ud83d\udcca Metrics                                         \u2502 \u2502\n\u2502  \u2502                                                     \u2502 \u2502\n\u2502  \u2502 \u23f1\ufe0f  Duration: 23.5s                                 \u2502 \u2502\n\u2502  \u2502 \ud83d\udd22 Steps: 12                                        \u2502 \u2502\n\u2502  \u2502 \ud83d\udee0\ufe0f  Tools: 18                                       \u2502 \u2502\n\u2502  \u2502 \ud83d\udce5 Input Tokens: 15,234                            \u2502 \u2502\n\u2502  \u2502 \ud83d\udce4 Output Tokens: 2,456                            \u2502 \u2502\n\u2502  \u2502 \ud83d\udcb0 Est. Cost: $0.0234                              \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u276f \u2588                                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 enter: submit task  \u2022  ctrl+u: clear  \u2022  q: quit        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"TUI_GUIDE/#running-state","title":"Running State","text":"<p>When a task is running, you'll see:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 \ud83c\udfaf Task: Fix the food spawning issue                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u23f1\ufe0f  12s  |  Step 8  |  Reading internal/game/actors.go\n\n\u280b Working...\n</code></pre>"},{"location":"TUI_GUIDE/#examples","title":"Examples","text":""},{"location":"TUI_GUIDE/#example-session","title":"Example Session","text":"<pre><code>$ ./dodo --repo ../myproject --tui\n\n\ud83e\udda4 DODO Interactive Agent          Tasks: 0\n\n\ud83d\udc4b Welcome! Enter a task below to get started.\n\n\u276f add error handling to the API endpoints\u2588\n\n# Agent works on the task...\n\n\u2705 Task Completed\n\nTask: add error handling to the API endpoints\n\nResult: I've added comprehensive error handling...\n\n\ud83d\udcca Metrics\n\u23f1\ufe0f  Duration: 18.2s\n\ud83d\udd22 Steps: 9\n\ud83d\udee0\ufe0f  Tools: 14\n\ud83d\udce5 Input Tokens: 12,456\n\ud83d\udce4 Output Tokens: 1,892\n\ud83d\udcb0 Est. Cost: $0.0189\n\n\u276f now add input validation\u2588\n\n# Continue with more tasks...\n</code></pre>"},{"location":"TUI_GUIDE/#benefits-over-single-task-mode","title":"Benefits Over Single-Task Mode","text":"Feature Single Task TUI Mode Multiple tasks \u274c Restart each time \u2705 Continuous session Metrics display \u2705 End only \u2705 Real-time + history Progress tracking \u274c Logs only \u2705 Live UI updates Task history \u274c None \u2705 Full history User experience \ud83d\ude10 CLI \ud83d\ude0d Interactive TUI"},{"location":"TUI_GUIDE/#technical-details","title":"Technical Details","text":"<ul> <li>Framework: Bubble Tea (Go TUI framework)</li> <li>Styling: Lip Gloss (terminal styling)</li> <li>State Management: Bubble Tea's Elm architecture</li> <li>Concurrency: Each task runs in its own context</li> <li>Cancellation: Graceful task cancellation with Ctrl+C</li> </ul>"},{"location":"TUI_GUIDE/#tips","title":"Tips","text":"<ol> <li>Use TUI for iterative development - Perfect for making multiple changes to a codebase</li> <li>Monitor token usage - See real-time cost estimates for each task</li> <li>Review history - Scroll through previous tasks and results</li> <li>Cancel long tasks - Press Ctrl+C to cancel without losing session</li> </ol>"},{"location":"TUI_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"TUI_GUIDE/#tui-not-displaying-correctly","title":"TUI not displaying correctly","text":"<ul> <li>Ensure your terminal supports ANSI colors and Unicode</li> <li>Try resizing your terminal window</li> <li>Use a modern terminal emulator (iTerm2, Alacritty, Windows Terminal)</li> </ul>"},{"location":"TUI_GUIDE/#task-not-starting","title":"Task not starting","text":"<ul> <li>Check that indexing completed successfully</li> <li>Verify your API keys are set (OPENAI_API_KEY, etc.)</li> <li>Look for error messages in the TUI</li> </ul>"},{"location":"TUI_GUIDE/#metrics-not-showing","title":"Metrics not showing","text":"<ul> <li>Metrics are estimated based on model pricing</li> <li>Some providers may not report token counts accurately</li> <li>Check that the model name is recognized</li> </ul>"},{"location":"TUI_GUIDE/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>[ ] Task queue management</li> <li>[ ] Export task history</li> <li>[ ] Custom themes</li> <li>[ ] Split-pane view for code + results</li> <li>[ ] Real-time file watching integration</li> <li>[ ] Task templates/favorites</li> </ul>"},{"location":"architecture/framework/","title":"The Dodo Framework","text":"<p>Dodo is not just an application; it is built on top of a powerful, custom-built agentic framework written in pure Go.</p>"},{"location":"architecture/framework/#why-a-custom-framework","title":"Why a Custom Framework?","text":"<p>When we started Dodo, we evaluated existing ecosystem options like LangChain (Python/Go) and various \"Agent\" libraries. We found them to be: 1.  Too Abstraacted: Hiding prompt logic and control flow behind layers of complexity. 2.  Heavy Dependencies: Importing massive dependency trees just to make an API call. 3.  Hard to Debug: When an agent loops or fails, understanding why inside a black-box framework is painful.</p> <p>We chose to build from scratch.</p>"},{"location":"architecture/framework/#core-philosophy-zero-dependencies","title":"Core Philosophy: \"Zero Dependencies\"","text":"<p>The Dodo Engine (<code>internal/engine</code>) is built with zero external functional dependencies. It relies only on the Go standard library and the official SDKs for LLM providers (OpenAI/Anthropic).</p> <p>This gives us: -   Binary Size: The entire compiled engine is ~15MB. -   Performance: Sub-millisecond overhead for agent reasoning steps. -   Stability: No \"dependency hell\" or breaking changes from upstream frameworks.</p>"},{"location":"architecture/framework/#architecture-internals","title":"Architecture Internals","text":"<p>The framework is Event-Driven and Hook-Based.</p>"},{"location":"architecture/framework/#the-agent-loop","title":"The Agent Loop","text":"<p>Instead of a complex graph, Dodo uses a modified ReAct loop with lifecycle hooks:</p> <pre><code>type Agent struct {\n    // ...\n    Hooks AgentHooks\n}\n\ntype AgentHooks struct {\n    OnStep       func(ctx Context, step Step)\n    OnToolStart  func(ctx Context, tool ToolCall)\n    OnToolFinish func(ctx Context, result ToolResult)\n}\n</code></pre> <p>This architecture allows us to attach: -   Real-time Reporting: Hooks stream events to the UI immediately. -   Safety Checks: A hook can intercept a <code>run_cmd</code> tool call and block it if it looks dangerous. -   Middleware: We can inject \"thoughts\" or memories into the context before the LLM sees them.</p>"},{"location":"architecture/framework/#interfaces","title":"Interfaces","text":"<p>Everything is an interface, making the system highly testable and extensible.</p> <p>The Tool Interface: <pre><code>type Tool interface {\n    Name() string\n    Description() string\n    Schema() jsonschema.Definition\n    Execute(ctx context.Context, args json.RawMessage) (any, error)\n}\n</code></pre></p> <p>The Provider Interface: <pre><code>type LLMProvider interface {\n    Stream(ctx context.Context, messages []Message) (&lt;-chan Event, error)\n}\n</code></pre></p>"},{"location":"architecture/framework/#future-standalone-library","title":"Future: Standalone Library","text":"<p>We believe this \"thin, fast, and transparent\" approach to AI agents is valuable beyond Dodo.</p> <p>Roadmap Item: We plan to extract <code>internal/engine</code> into a standalone library (<code>github.com/dodo-ai/framework</code>) so other Go developers can build: -   Custom coding assistants. -   DevOps bots. -   Customer support agents.</p> <p>All with the same performance and safety guarantees as Dodo.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Coming Soon...</p>"},{"location":"architecture/protocol/","title":"Protocol","text":"<p>Coming Soon...</p>"},{"location":"architecture/tools/","title":"Custom Tools","text":"<p>Coming Soon...</p>"},{"location":"concepts/agentic-workflow/","title":"The Agentic Workflow","text":"<p>Unlike autocomplete tools (like GitHub Copilot) that predict the next few lines of text, Dodo behaves like a semi-autonomous developer. It uses a structured loop of reasoning to solve large-scale problems.</p>"},{"location":"concepts/agentic-workflow/#the-loop-think-plan-act","title":"The Loop: Think \u2192 Plan \u2192 Act","text":"<p>When you give Dodo a task, it doesn't just start writing code. It follows a rigorous mental process:</p> <pre><code>graph TD\n    User[User Request] --&gt; Think[Thinking &amp; Analysis]\n    Think --&gt; Plan[Create/Update Plan]\n    Plan --&gt; Tool[Execute Tool]\n    Tool --&gt; Result[Tool Result]\n    Result --&gt; Think\n    Think --&gt; Done[Task Completion]</code></pre>"},{"location":"concepts/agentic-workflow/#1-thinking-reasoning","title":"1. Thinking (Reasoning)","text":"<p>Before touching any file, the agent analyzes the request. It asks: - \"Do I have enough context?\" - \"What files are relevant?\" - \"Is this risky?\"</p> <p>This internal monologue is streamed to you in real-time, so you know exactly why the agent is making a decision.</p>"},{"location":"concepts/agentic-workflow/#2-planning","title":"2. Planning","text":"<p>For complex tasks (like \"Refactor this module\"), Dodo creates a structured plan.  - It breaks the goal into step-by-step milestones. - It won't proceed to step 2 until step 1 is verified. - You are involved: You can see the plan and (in future versions) edit it before execution.</p>"},{"location":"concepts/agentic-workflow/#3-tool-execution","title":"3. Tool Execution","text":"<p>Dodo interacts with your computer using Tools. It cannot do anything \"magic\" outside of these defined capabilities: - <code>read_file</code>: Read file contents. - <code>codebase_search</code>: Find code semantic search. - <code>run_cmd</code>: Execute terminal commands (tests, builds). - <code>replace_file_content</code>: Edit code with surgical precision.</p>"},{"location":"concepts/agentic-workflow/#4-verification","title":"4. Verification","text":"<p>Crucially, Dodo checks its work. After editing a file, it might run the build command or the unit tests to ensure it didn't break anything. If verification fails, it self-corrects and tries again.</p>"},{"location":"concepts/agentic-workflow/#why-this-matters","title":"Why this matters?","text":"<p>This workflow allows Dodo to handle tasks that require long-term memory and multi-step reasoning, such as: - \"Upgrade this dependency and fix all breaking changes.\" - \"Implement this feature across the backend and frontend.\" - \"Debug this error by adding logs, running the app, and reading the output.\"</p>"},{"location":"concepts/indexing/","title":"Semantic Indexing","text":"<p>Coming Soon...</p>"},{"location":"concepts/sandboxing/","title":"Sandboxed Execution","text":"<p>Security is a primary concern when letting an AI agent execute commands on your machine. Dodo uses Docker containers to sandbox risky operations, ensuring the agent cannot accidentally (or maliciously) harm your system.</p>"},{"location":"concepts/sandboxing/#how-it-works","title":"How it Works","text":"<p>When Dodo needs to run a shell command (e.g., via <code>run_cmd</code> or <code>run_tests</code>), it checks your <code>sandbox</code> configuration.</p>"},{"location":"concepts/sandboxing/#auto-mode-sandbox_mode-auto","title":"Auto Mode (<code>sandbox_mode: auto</code>)","text":"<p>This is the default and recommended mode.</p> <ol> <li>Detection: Dodo checks if the Docker daemon is reachable.</li> <li>Containerization: If Docker is found, Dodo spins up an ephemeral container tailored to your project language (e.g., <code>golang:alpine</code> for Go, <code>node:alpine</code> for JS).</li> <li>Isolation:<ul> <li>The command runs inside the container, not on your host shell.</li> <li>Your project directory is mounted as a volume so the agent can see your files.</li> <li>The container has no network access (by default) and restricted capabilities.</li> </ul> </li> <li>Cleanup: The container is destroyed immediately after the command finishes.</li> </ol>"},{"location":"concepts/sandboxing/#host-mode-sandbox_mode-host","title":"Host Mode (<code>sandbox_mode: host</code>)","text":"<p>If Docker is not available (or explicitly disabled), Dodo falls back to running commands directly on your host operating system shell (zsh/bash/powershell).</p> <p>Security Risk</p> <p>In Host Mode, the agent has the same permissions as your user user. It could theoretically run <code>rm -rf ~</code> or upload your SSH keys if instructed by a malicious prompt injection. Use Host Mode only with trusted models and verified prompts.</p>"},{"location":"concepts/sandboxing/#configuration","title":"Configuration","text":"<p>You can configure sandboxing per-session or globally in <code>~/.dodo/config.yaml</code>.</p> <pre><code>sandbox:\n  mode: auto      # Options: auto, docker, host\n  timeout: 300    # Kill commands after 5 minutes\n\n  # Advanced: Custom container image\n  image: my-custom-build-image:latest\n</code></pre>"},{"location":"contributing/development/","title":"Development Guide","text":"<p>Want to contribute to Dodo? This guide will help you set up your development environment.</p>"},{"location":"contributing/development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Go 1.21+: Required to build the engine.</li> <li>Node.js 18+: Required to build the Ink CLI.</li> <li>Docker: Optional (but recommended) for testing sandboxed execution.</li> </ul>"},{"location":"contributing/development/#monorepo-structure","title":"Monorepo Structure","text":"<p>Dodo is a monorepo containing both the backend and frontend:</p> <pre><code>dodo/\n\u251c\u2500\u2500 cmd/            # Go entrypoints\n\u251c\u2500\u2500 internal/       # Go engine source code\n\u251c\u2500\u2500 ink-ui/         # React/Ink CLI source code\n\u251c\u2500\u2500 npm-package/    # The publishing artifacts for npm\n\u2514\u2500\u2500 docs/           # This website\n</code></pre>"},{"location":"contributing/development/#running-locally","title":"Running Locally","text":""},{"location":"contributing/development/#1-build-the-engine","title":"1. Build the Engine","text":"<p>The React UI needs the Go binary to be built first.</p> <pre><code># In the root 'dodo/' directory\ngo build -o repl ./cmd/repl\n</code></pre> <p>This creates a <code>repl</code> binary in your root folder.</p>"},{"location":"contributing/development/#2-run-the-ui-in-dev-mode","title":"2. Run the UI in Dev Mode","text":"<p>Navigate to the UI directory and start the dev server. We pass the <code>--engine</code> flag to tell it where to find the binary we just built.</p> <pre><code>cd ink-ui\nnpm install\n\n# Run dev mode (with hot reloading for UI components)\nnpm run dev -- --engine ../repl\n</code></pre>"},{"location":"contributing/development/#3-debugging","title":"3. Debugging","text":"<p>You can enable verbose debug logs to see exactly what JSON triggers are being sent back and forth.</p> <pre><code>export DODO_DEBUG=true\nnpm run dev -- --engine ../repl\n</code></pre> <p>Logs will be written to <code>/tmp/dodo_debug.log</code>.</p>"},{"location":"contributing/development/#running-tests","title":"Running Tests","text":""},{"location":"contributing/development/#go-tests-engine","title":"Go Tests (Engine)","text":"<pre><code>go test ./...\n</code></pre>"},{"location":"contributing/development/#typescript-tests-ui","title":"TypeScript Tests (UI)","text":"<pre><code>cd ink-ui\nnpm test\n</code></pre>"},{"location":"contributing/releasing/","title":"Release Process","text":"<p>Coming Soon...</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Dodo is designed to be flexible. You can configure it to use almost any LLM provider and customize safety settings.</p>"},{"location":"getting-started/configuration/#the-setup-wizard-recommended","title":"The Setup Wizard (Recommended)","text":"<p>The easiest way to configure Dodo is using the built-in interactive wizard.</p> <ol> <li>Start Dodo:     <pre><code>dodo\n</code></pre></li> <li>Type <code>/conf</code> in the input bar (or select Configure from the main menu starting screen).</li> </ol> <p>The wizard will guide you through: -   Selecting a Provider: OpenAI, Anthropic, Gemini, or Local. -   Entering API Keys: Securely stored on your device. -   Sandbox Settings: Enabling Docker for safe execution.</p> <p>Configuration is saved to <code>~/.dodo/config.yaml</code>.</p>"},{"location":"getting-started/configuration/#manual-configuration","title":"Manual Configuration","text":"<p>You can also create or edit the config file manually at <code>~/.dodo/config.yaml</code>.</p>"},{"location":"getting-started/configuration/#llm-providers","title":"LLM Providers","text":"OpenAIAnthropicLocal / Ollama <pre><code>llm:\n  provider: openai\n  api_key: sk-your-api-key\n  model: gpt-4o\n</code></pre> <pre><code>llm:\n  provider: anthropic\n  api_key: sk-ant-your-key\n  model: claude-3-5-sonnet-20240620\n</code></pre> <p>You can use any OpenAI-compatible endpoint (like Ollama or LM Studio).</p> <pre><code>llm:\n  provider: openai\n  base_url: http://localhost:11434/v1\n  api_key: ollama  # Value doesn't matter for Ollama, but must be present\n  model: llama3\n</code></pre>"},{"location":"getting-started/configuration/#safety-sandboxing","title":"Safety &amp; Sandboxing","text":"<p>Dodo strongly recommends using Docker to sandbox the agent's execution. This prevents the agent from accidentally (or intentionally) modifying files outside your project or installing unwanted software.</p> <p>To enable sandboxing:</p> <ol> <li>Ensure Docker Desktop is installed and running.</li> <li>Set <code>sandbox_mode: auto</code> in your config.</li> </ol> <pre><code>sandbox:\n  mode: auto  # Use \"docker\" to force fail if docker is missing\n</code></pre> <p>Without Sandbox</p> <p>If you disable sandboxing (<code>mode: host</code>), the agent runs commands properly on your machine shell. Only use this with trusted models.</p>"},{"location":"getting-started/first-steps/","title":"First Steps","text":"<p>Coming Soon...</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Dodo consists of two parts: 1.  The CLI (NPM): A React-based terminal UI. 2.  The Engine (Go): A high-performance agent runtime (automatically managed by the CLI).</p>"},{"location":"getting-started/installation/#recommended-npm-install","title":"Recommended: NPM Install","text":"<p>The easiest way to install Dodo is via <code>npm</code>. This installs the CLI wrapper which will automatically download the correct engine binary for your OS (macOS, Linux, or Windows).</p> <pre><code>npm install -g dodo-ai\n</code></pre> <p>Verify the installation:</p> <pre><code>dodo --version\n</code></pre> <p>Ready to go!</p> <p>You can now run <code>dodo</code> in any directory. The first time you run it, it will download the ~15MB engine binary.</p>"},{"location":"getting-started/installation/#alternative-build-from-source","title":"Alternative: Build from Source","text":"<p>If you prefer to build the engine yourself (e.g., for contributing), you'll need Go 1.21+ and Node.js 18+.</p> <ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/ChamsBouzaiene/dodo.git\ncd dodo\n</code></pre></p> </li> <li> <p>Build the Engine: <pre><code>go build -o repl ./cmd/repl\n</code></pre></p> </li> <li> <p>Run the UI: <pre><code>cd ink-ui\nnpm install\nnpm run dev -- --engine ../repl\n</code></pre></p> </li> </ol>"},{"location":"guides/advanced/","title":"Advanced Usage","text":"<p>Coming Soon...</p>"},{"location":"guides/cli-reference/","title":"CLI Reference","text":"<p>Dodo's interface is designed to be fully controllable from the keyboard.</p>"},{"location":"guides/cli-reference/#slash-commands","title":"Slash Commands","text":"<p>Type these commands into the input bar to control the session.</p> Command Description <code>/help</code> Show a list of available commands and their descriptions. <code>/clear</code> Clear Context: Wipe the conversation history but keep the current project/configuration. Useful when the context window gets full. <code>/conf</code> Configure: Open the interactive Setup Wizard to change LLM providers or API keys. <code>/mode</code> Switch Mode: Toggle between <code>Planner</code>, <code>Code</code>, and <code>Architect</code> modes. <code>/tools</code> Tools Viewer: List all enabled tools for the current session and their status. <code>/index</code> Re-index: Force a full re-scan of the codebase for the semantic search index. <code>/debug</code> Diagnostics: Dump the current internal state to a temp file (useful for bug reports). <code>/exit</code> Quit: Safely shutdown the session and cleanup any resources."},{"location":"guides/cli-reference/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"Shortcut Action <code>Metx + C</code> Copy: Copy selected text or code block. <code>Ctrl + C</code> Cancel / Exit: Interrupt the current agent action or exit the application. <code>Up / Down</code> History: Navigate through your previous inputs in the prompt bar. <code>Enter</code> Submit: Send your message to the agent."},{"location":"guides/cli-reference/#launch-arguments","title":"Launch Arguments","text":"<p>When starting <code>dodo</code> from the terminal, you can pass flags:</p> <pre><code>dodo [flags]\n</code></pre> <ul> <li><code>--repo &lt;path&gt;</code>: Open a specific repository (defaults to current directory).</li> <li><code>--debug</code>: Enable verbose debug logging to stderr.</li> <li><code>--version</code>: Print the current version.</li> </ul>"},{"location":"guides/modes/","title":"Operating Modes","text":"<p>Dodo isn't one-size-fits-all. Different tasks require different \"mindsets.\" Dodo supports specialized Modes that change the system prompt and available tools.</p> <p>Switch modes anytime using the <code>/mode</code> command.</p>"},{"location":"guides/modes/#1-coder-mode-default","title":"1. Coder Mode (Default)","text":"<p>Goal: Implementation &amp; Fixes.</p> <p>This is the standard day-to-day mode. The agent assumes it is a Senior Software Engineer paired with you. </p> <ul> <li>Strengths: Writing code, refactoring, fixing bugs, running tests.</li> <li>Tools: Full access (Read, Write, Execute).</li> <li>Behavior: Biased towards action. It will try to solve the problem by editing files.</li> </ul>"},{"location":"guides/modes/#2-architect-mode","title":"2. Architect Mode","text":"<p>Goal: High-Level Design &amp; Planning.</p> <p>Use this mode when you are starting a complex feature and want to discuss the approach before writing code.</p> <ul> <li>Strengths: System design, directory structure planning, technology choices.</li> <li>Tools: Read-only (mostly). It can read files to understand the current state but is discouraged from making edits.</li> <li>Behavior: Biased towards discussion and creating <code>implementation_plan.md</code> artifacts.</li> </ul>"},{"location":"guides/modes/#3-ask-mode","title":"3. Ask Mode","text":"<p>Goal: Q&amp;A and Exploration.</p> <p>Use this mode when you just want to understand the codebase without any risk of changing it.</p> <ul> <li>Strengths: \"Where is the auth logic defined?\", \"How does the billing system work?\".</li> <li>Tools: Read-only (Search, Read). Write tools are disabled.</li> <li>Behavior: Purely conversational. It acts as an expert guide to your repository.</li> </ul>"}]}